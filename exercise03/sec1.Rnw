\section*{Problem 1}
\subsection*{a}
We assume that the Moran process is described by a population of size \( N \) and a tridiagonal transition matrix \( \mathbf{P} = (P_{i,j}) = \Pr[X(t + 1) = j \mid X(t) = i] \). Assuming that at time \( t \) we are at state \( i \), we calculate the expected value of the next state:
\begin{align*}
    \E[X(t + 1) \mid X(t) = i]
        &= \sum_{j = i - 1}^{i + 1} j P_{i, j} \\
        &= (i - 1) P_{i, i - 1} + i P_{i, i} + (i + 1) P_{i, i + 1} \\
        &= i (P_{i, i - 1} + P_{i, i} + P_{i, i + 1}) - P_{i, i - 1} + P_{i, i + 1} \\
        &= i - \frac{i}{N} \frac{N - i}{N} + \frac{N - i}{N} \frac{i}{N} &[\mbox{\( \mathbf{P} \) is row-stochastic}] \\
        &= i
\end{align*}
Applying the law of total expectation, \( \E_{Y}[Y] = \E_{Z}[\E_{Y}[Y \mid Z]] \) for \( Y = X(t + 1) \) and \( Z = X(t) \):
\begin{align*}
    \E[X(t + 1) \mid X(t)]
        &= \E[\E[X(t + 1) \mid X(t) = i]] \\
        &= \sum_{i = 0}^{N} \E[X(t + 1) \mid X(t) = i] \Pr[X(t) = i] \\
        &= \sum_{i = 0}^{N} i \Pr[X(t) = i] \\
        &= \E[X(t)] &[\mbox{by definition of \( \E[\cdot] \)}]
\end{align*}
The Moran process is a time-homogeneous Markov process and therefore
\[
    \forall t > 0: \Pr[X(t + 1) = j \mid X(t) = i] = \Pr[X(1) = j \mid X(0) = i] = P_{i, j}
\]
that is, the distribution of  \( X(t + 1) \) given \( X(t) \) does not depend on \( t \). This extends to aggregate measures of this distribution, for example the expected value and the variance. Thus:
\[
    \forall t > 0: \E[X(t + 1) \mid X(t) = i] = \E[X(1) \mid X(0) = i]
\]
We can now use induction to prove
\[
    \forall t \geq 0: \E[X(t) \mid X(0) = i] = i
\]
It holds trivially for \( t = 0 \). Suppose the formula holds for all nonnegative \( t \) less than \( s \). By using the law of total expectation, we have:
{\small
\begin{align*}
    \E[X(s) \mid X(0) = i]
        &= \sum_{q = 0}^{N} \E[X(s) \mid X(s - 1) = q, X(0) = i] \Pr[X(s - 1) = q \mid X(0) = i] \\
        &= \sum_{q = 0}^{N} \E[X(s) \mid X(s - 1) = q] \Pr[X(s - 1) = q \mid X(0) = i] &[\mbox{Markov property}] \\
        &= \sum_{q = 0}^{N} q \Pr[X(s - 1) = q \mid X(0) = i] \\
        &= \E[X(s - 1) \mid X(0) = i] \\
        &= i &[\mbox{inductive hypothesis}]
\end{align*}
}
Since we proved the base step and the inductive step, the proof is complete.

\subsection*{b}
\subsubsection*{i}
Similar to the expression for \( \E \), we have:
\[
    \forall t > 0: \Var[X(t + 1) \mid X(t) = i] = \Var[X(1) \mid X(0) = i]
\]
We compute \( \Var[X(1) \mid X(0) = i] \) by utilizing the definition of variance:
\begin{align*}
    \Var[X(1) \mid X(0) = i]
        &= \E[X^2(1) \mid X(0) = i] - (\E[X(1) \mid X(0) = i])^2 \\
        &= \del{\sum_{j = i - 1}^{i + 1} j^2 P_{i, j}} - i^2 \\
        &= (i - 1)^2 P_{i, i - 1} + i^2 P_{i, i} + (i + 1)^2 P_{i, i + 1} - i^2 \\
        &= i^2 (P_{i, i - 1} + P_{i, i} + P_{i, i + 1}) - (2i - 1) P_{i, i - 1} + (2i + 1) P_{i, i + 1} - i^2 \\
        &= i^2 - (2i - 1) \frac{i}{N} \frac{N - i}{N} + (2i + 1) \frac{N - i}{N} \frac{i}{N} - i^2 \\
        &= 2 \frac{i(N - i)}{N^2} \\
        &= 2 i/N \del{1 - i/N} \\
        &= V_1(i)
\end{align*}
\subsubsection*{ii}
We will prove that for all \( t \geq 0 \)
\[
    \Var[X(t + 1) \mid X(0) = i] = V_1(i) + (1 - 2/N^2) \Var[X(t) \mid X(0) = i]
\]
By using the law of total variance and substituting \( Y = X(t + 1) \mid X(0) = i \) and \( Z = X(t) \), we get for each of the components:
{\small
\begin{align*}
    \E_{X(t)}[\Var_{X(t + 1)}[X(t + 1) \mid X(t), X(0) = i]]
        &= \E_{X(t)}[\Var_{X(t + 1)}[X(t + 1) \mid X(t), X(0) = i]] \\
        &= \E_{X(t)}[2 X(t) / N (1 - X(t) / N) \mid X(0) = i] \\
        &= - 2/N^2 \E_{X(t)}[X^2(t)] + 2/N \E_{X(t)}[X(t) \mid X(0) = i] \\
        &= - 2/N^2 (\Var_{X(t)}[X(t) \mid X(0) = i] + (\E_{X(t)}[X(t) \mid X(0) = i])^2) + 2i/N \\
        &= - 2/N^2 (\Var_{X(t)}[X(t) \mid X(0) = i] + i^2) + 2i/N \\
        &= - 2/N^2 \Var_{X(t)}[X(t) \mid X(0) = i] + 2i/N(1 - i/N) \\
        &= - 2/N^2 \Var_{X(t)}[X(t) \mid X(0) = i] + V_1(i) \\
    \\
    \Var_{X(t)}[\E_{X(t + 1)}[X(t + 1) \mid X(t), X(0) = i]]
        &= \Var_{X(t)}[X(t) \mid X(0) = i]
\end{align*}
}
and thus:
\begin{align*}
    \Var_{X(t + 1)}
        &= \E_{X(t)}[\Var_{X(t + 1)}[X(t + 1) \mid X(t), X(0) = i]] + \Var_{X(t)}[\E_{X(t + 1)}[X(t + 1) \mid X(t), X(0) = i]] \\
        &= - 2/N^2 \Var_{X(t)}[X(t) \mid X(0) = i] + V_1(i) + \Var_{X(t)}[X(t) \mid X(0) = i] \\
        &= V_1(i) + (1 - 2/N^2) \Var_{X(t)}[X(t) \mid X(0) = i]
\end{align*}

\subsubsection*{iii}
We have:
\begin{align*}
    V_1(i)
        &= 2\frac{i(N - i)}{N^2} \\
        &= i (N - i)\del{1 - 1 + 2/N^2} \\
        &= - i (N - i)\del{1 - 2/N^2} + i(N - i)
\end{align*}
and thus if \( x_t(i) = \Var[X(t) \mid X(0) = i] \), the equation for \( \Var[X(t) \mid X(0) = 1] \) becomes:
\begin{align*}
    x_t(i) &= V_1(i) + (1 - 2/N^2) x_{t - 1}(i) \\
    x_t(i) &= - i (N - i)(1 - 2/N^2) + i(N - i) + (1 - 2/N^2) x_{t - 1}(i) \\
    x_t(i) - i(N - i) &= (1 - 2/N^2) (x_{t - 1}(i) - i(N - i))
\end{align*}
which is in the desired form.

Therefore:
\begin{align*}
    x_t(i) &= i(N - i) + \del{1 - 2/N^2}^{t - 1} (x_{1}(i) - i(N - i)) \\
           &= i(N - i) + \del{1 - 2/N^2}^{t - 1} (V_{1}(i) - i(N - i)) \\
           &= i(N - i) + \del{1 - 2/N^2}^{t - 1} (2i(N - i)/N^2 - i(N - i)) \\
           &= i(N - i) + \del{1 - 2/N^2}^{t - 1} i(N - i) (2/N^2 - 1) \\
           &= i(N - i) \del{1 - \del{1 - 2/N^2}^t} \\
           &= N^2 V_1(i)/2 \del{1 - \del{1 - 2/N^2}^t} \\
           &= V_1(i) \frac{1 - \del{1 - 2/N^2}^t}{2 / N^2}
\end{align*}

\subsection*{c}
Let
\[
    f(N) = V_1 \frac{1 - (1 - 2/N^2)^t}{2/N^2}
\]
When \( N \gg 1 \), then \( 2/N^2 \ll 1 \) and thus we can employ the binomial approximation:
\begin{align*}
    f(N)
        &= V_1 \frac{1 - (1 - 2/N^2)^t}{2/N^2} \\
        &\approx V_1 \frac{1 - (1 - 2t/N^2)}{2/N^2} \\
        &= V_1 \frac{2t/N^2}{2/N^2} \\
        &= V_1 t
\end{align*}
