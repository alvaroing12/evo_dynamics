\section*{Problem 1}
\subsection*{a}
We assume that the Moran process is described by a population of size \( N \) and a tridiagonal transition matrix \( \mathbf{P} = (P_{i,j}) = \Pr[X(t + 1) = j \mid X(t) = i] \). Assuming that at time \( t \) we are at state \( i \), we calculate the expected value of the next state:
\begin{align*}
    \E[X(t + 1) \mid X(t) = i]
        &= \sum_{j = i - 1}^{i + 1} j P_{i, j} \\
        &= (i - 1) P_{i, i - 1} + i P_{i, i} + (i + 1) P_{i, i + 1} \\
        &= i (P_{i, i - 1} + P_{i, i} + P_{i, i + 1}) - P_{i, i - 1} + P_{i, i + 1} \\
        &= i - \frac{i}{N} \frac{N - i}{N} + \frac{N - i}{N} \frac{i}{N} &[\mbox{\( \mathbf{P} \) is row-stochastic}] \\
        &= i
\end{align*}
Applying the law of total expectation, \( \E_{Y}[Y] = \E_{Z}[\E_{Y}[Y \mid Z]] \) for \( Y = X(t + 1) \) and \( Z = X(t) \):
\begin{align*}
    \E[X(t + 1) \mid X(t)]
        &= \E[\E[X(t + 1) \mid X(t) = i]] \\
        &= \sum_{i = 0}^{N} \E[X(t + 1) \mid X(t) = i] \Pr[X(t) = i] \\
        &= \sum_{i = 0}^{N} i \Pr[X(t) = i] \\
        &= \E[X(t)] &[\mbox{by definition of \( \E[\cdot] \)}]
\end{align*}
The Moran process is a memoryless time-homogenous Markov process and therefore
\[
    \forall t > 0: \Pr[X(t + 1) = j \mid X(t) = i] = \Pr[X(1) = j \mid X(0) = i] = P_{i, j}
\]
that is, the distribution of  \( X(t) \) given \( X(t - 1) \) does not depend on \( t \). This extends to aggregate measures of this distribution, for example the expected value and the variance. Thus:
\[
    \forall t > 0: \E[X(t + 1) \mid X(t) = i] = \E[X(1) \mid X(0) = i]
\]
We can now prove the formula requested by induction on \( t \). For \( t = 0 \), it trivially holds that
\[
    \E[X(0) | X(0) = i] = i
\]
Suppose the formula holds for all nonnegative \( t \) less than \( s \). Then:
\footnotesize
\begin{align*}
    \E[X(s) \mid X(0) = i]
        &= \sum_{j = 0}^{N} j \Pr[X(s) = j \mid X(0) = i] \\
        &= \sum_{q = 0}^{N} \sum_{j = 0}^{N} j \Pr[X(s) = j \mid X(s - 1) = q, X(0) = i] \Pr[X(s - 1) = q \mid X(0) = i]\\
        &= \sum_{q = 0}^{N} \sum_{j = 0}^{N} j \Pr[X(s) = j \mid X(s - 1) = q] \Pr[X(s - 1) = q \mid X(0) = i] &[\mbox{Markov property}] \\
        &= \sum_{q = 0}^{N} \Pr[X(s - 1) = q \mid X(0) = i] \sum_{j = 0}^{N} j \Pr[X(s) = j \mid X(s - 1) = q] \\
        &= \sum_{q = 0}^{N} \Pr[X(s - 1) = q \mid X(0) = i] \E[X(s) \mid X(s - 1) = q] \\
        &= \sum_{q = 0}^{N} \Pr[X(s - 1) = q \mid X(0) = i] \E[X(1) \mid X(0) = q] \\
        &= \sum_{q = 0}^{N} q \Pr[X(s - 1) = q \mid X(0) = i] \\
        &= \E[X(s - 1) \mid X(0) = i] \\
        &= i &[\mbox{inductive hypothesis}]
\end{align*}
\normalsize
Since we proved the base step and the inductive step, the proof is complete.

\subsection*{b}
\subsubsection*{i}
\[
    \forall t > 0: \Var[X(t + 1) \mid X(t) = i] = \Var[X(1) \mid X(0) = i]
\]
We note that \( \Var[X] = \E[(X - \E[X])^2] = \E[X^2] - \E[X]^2 \) and proceed as follows:
\begin{align*}
    \E[X^2(1) \mid X(0) = i]
        &= \sum_{j = i - 1}^{i + 1} j^2 P_{i, j} \\
        &= (i - 1)^2 P_{i, i - 1} + i^2 P_{i, i} + (i + 1)^2 P_{i, i + 1} \\
        &= i^2 (P_{i, i - 1} + P_{i, i} + P_{i, i + 1}) - (2i - 1) P_{i, i - 1} + (2i + 1) P_{i, i + 1} \\
        &= i^2 - (2i - 1) \frac{i}{N} \frac{N - i}{N} + (2i + 1) \frac{N - i}{N} \frac{i}{N} \\
        &= i^2 + 2 \frac{i(N - i)}{N^2} \\
        &= i^2 + 2 \frac{i}{N} \frac{N - i}{N} \\
    \\
    \E[X(1) \mid X(0) = i] &= i \\
    \\
    V_1(i)
        &= \Var[X(1) \mid X(0) = i] \\
        &= \E[X^2(1) \mid X(0) = i] - (\E[X(1) \mid X(0) = i])^2 \\
        &= i^2 + 2 \frac{i}{N} \frac{N - i}{N} - i^2 \\
        &= 2 \frac{i}{N} \frac{N - i}{N} \\
        &= 2 i/N \del{1 - i/N}
\end{align*}
\subsubsection*{ii}
To derive the variance of \( X(t) \) given \( X(0) = i \) we use the law of total variance,
\[
    \Var[Y] = \E_Z[\Var_Y[Y \mid Z]] + \Var_Z[\E_Y[Y \mid Z]]
\]
and substitute, as before, \( Y = X(t + 1) \) and \( Z = X(t) \):
\[
    \Var[Y] = \E_Z[\Var_Y[Y \mid Z]] + \Var_Z[\E_Y[Y \mid Z]]
\]
\begin{align*}
    \Var_Y[Y \mid Z = i]
        &= \Var[X(t + 1) \mid X(t) = i] \\
        &= \Var[X(1) \mid X(0) = i] \\
        &= V_1(i) \\
    \\
    \E_Z[\Var_Y[Y \mid Z]]
        &=\sum_{i = 0}^{N} \Pr[Z = i] \Var_Y[Y \mid Z = i] \\
        &=\sum_{i = 0}^{N} \Pr[X(t) = i] \Var[X(t + 1) \mid X(t) = i] \\
        &=\sum_{i = 0}^{N} \Pr[X(t) = i] V_1(i) \\
        &=2/N\sum_{i = 0}^{N} \Pr[X(t) = i] \del{i - i^2/N} \\
        &=2/N(\E[X(t)] - \E[X^2(t)]/N)
    \\
    \E_Y[Y \mid Z = i]
        &= \E[X(t + 1) \mid X(t) = i] \\
        &= i \\
    \\
    \Var_Z[\E_Y[Y \mid Z]]
        \E_Z[(\E_Y[Y \mid Z])^2] - (\E_Z[\E_Y[Y \mid Z]])^2 \\
        \E_Z[(\E_Y[Y \mid Z])^2] - (\E_Y[Y])^2 \\
        \E[(\E[X(t + 1) \mid X(t)])^2] - (\E[X(t + 1)])^2 \\
        \E[(\E[X(t))^2] - (\E[X(t + 1)])^2 \\
        &=
\end{align*}

\subsubsection*{iii}


\subsection*{c}
Let
\[
    f(N) = V_1 \frac{1 - (1 - 2/N^2)^t}{2/N^2}
\]
When \( N \gg 1 \), then \( 2/N^2 \ll 1 \) and thus we can employ the binomial approximation:
\begin{align*}
    f(N)
        &= V_1 \frac{1 - (1 - 2/N^2)^t}{2/N^2} \\
        &\approx V_1 \frac{1 - (1 - 2t/N^2)}{2/N^2} \\
        &= V_1 \frac{2t/N^2}{2/N^2} \\
        &= V_1 t
\end{align*}
